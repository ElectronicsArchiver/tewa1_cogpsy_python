{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7836b7e4-afb3-4af3-9c89-a09b94862926",
   "metadata": {
    "id": "aAmuxCstDaa3"
   },
   "source": [
    "homework 11\n",
    "===\n",
    "due: June 15 2022\n",
    "\n",
    "authors: pegler, prüwasser, scheftner\n",
    "\n",
    "---\n",
    "Read chapter 1-2 of Bayesian models of perception and action.\n",
    "\n",
    "https://www.cns.nyu.edu/malab/static/files/Bayesian_models_of_perception_and_action_v3.pdf\n",
    "\n",
    "Choose 6 problems from the end of the chapter, for both chapters, and submit the solution as text here.\n",
    "\n",
    "No need to write long explanations, try to be brief!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef562f40-8684-43f5-963b-cffc40e56d4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chapter 1\n",
    "\n",
    "### Summary\n",
    "In this chapter, we have introduced the concept that perception is inherently probabilistic, and as\n",
    "such optimally characterized as a process of Bayesian inference. Regarding Bayesian inference, we\n",
    "have learned the following:\n",
    "\n",
    "* Conditional probabilities such as $p(A | B)$ represents the probability of $A$ given $B$. In Bayesian\n",
    "perceptual inference, $A$ and $B$ typically represent a world state and an observation.\n",
    "* The likelihood function, $p(observation | world state)$, captures the information content of the\n",
    "sensory observation, relevant to distinguishing one world state from another.\n",
    "* <mark>The flatter the likelihood function, the less we learn from our senses</mark>. If the likelihood\n",
    "function is perfectly flat, then the observer has learned nothing from the observation.\n",
    "* In some cases, such as the “Is that my friend?” example, the likelihood changes over time.\n",
    "* <mark>The prior distribution over world states, $p(world state)$, summarizes the information content\n",
    "of our past observations</mark>, the background knowledge we have about the world. Perception\n",
    "is not based entirely on sensory observation, but also on expectation grounded in previous\n",
    "experience.\n",
    "* <mark>Flatter prior distributions mean we know less</mark> about the potential states of the world.\n",
    "* Bayes’ rule calculates the posterior probability of each hypothesized world state, $p(world state | observation)$,\n",
    "from the likelihoods and prior probabilities of the world states.\n",
    "* The procedures of Bayesian inference apply equally to situations in which the hypothesized\n",
    "world states are <mark>discrete</mark> or in which they are <mark>continuous</mark>.\n",
    "* Perceptual situations, whether in vision, audition, or other senses, are subject to various\n",
    "levels of <mark>uncertainty</mark>.\n",
    "* Speech perception is fraught with phonetic and syntactic ambiguity, frequently giving rise to\n",
    "flat likelihood functions. The combination of priors and likelihoods can cause misinterpretations such as mondegreens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa211998-2b8c-4e7e-b415-5a7bc16037ff",
   "metadata": {},
   "source": [
    "### Problem 1.1 Red roses in the garden\n",
    "Assuming that we, as the observer are able to perceive the garden perfectly fine and can process the imagery somewhat correctly (non-flat likelihood distribution):\n",
    "Being able to detect a rose more easily by it’s red color rather than green myrtle by green color mostly comes down to a stronger dependent prior. With prior knowledge that p(most plants that are red are roses) and p(there are several plants that are green) red roses are more easily discernible whereas the same does not hold true for green myrtle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6f6c0-434d-44a0-8226-c0a585036f17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problem 1.2 Why is it that we identify ourselves ...\n",
    "... at the very beginning of a phone conversation,\n",
    "even to people we already know, but we do not do this when we meet in person? Express your\n",
    "answer within the framework of Bayesian perceptual inference.\n",
    "\n",
    "**We know that in phone calls perception is limited to the auditory system and the quality of sensory data could be further limited by a bad phone connection. This potential noisiness we can counteract by telling the other person our name to reduce uncertainty and widen the likelihood function. When we meet someone in person we have much more and less ambigious sensory data at hand. From afar it might be still ambigious, but once we meet and see the other person from 2 or 3 meter distance we can be very certain that the other person recognizes us.**\n",
    "\n",
    "&\n",
    "\n",
    "**Person can fungate as prior and therefor influence perceived likelihood. Identifying oneself sets ankers for selection of priors (contexts), which will even the way of understanding correctly what a person might have said (likelihood) and therefor perceiving correctly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29df5f-ed3f-4c95-a23c-0eb8223a83dd",
   "metadata": {},
   "source": [
    "### Problem 1.3 Why we look at someones lips during conversation at a noisy party\n",
    "Why we will look at a speakers mouth in a noisy environment can be answered two ways:\n",
    "1.\tWe iteratively integrate Information creating a prior based on auditory information alone, which results in a rather flat likelihood curve. Using the resulting prior as the posterior and adding in the information we perceive with our eyes(the new likelihood) narrows down the amount of possible solutions and gives us a more accurate posterior.\n",
    "2.\tAlternatively we can think of both sensory inputs as two likelihoods, used at the same time, being averaged to unite both forms of information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31317b8b-d35c-4930-90a7-0bd4ba944695",
   "metadata": {},
   "source": [
    "### Problem 1.4 To explore how a noisy environment ...\n",
    "...engenders uncertainty, consider the word “lunch.” Suppose that you see this word written (or hear it spoken), with the letter “l” blocked out (making it unknown): _unch (e.g., by ambient auditory noise). List all source words that are compatible with what you see. Now consider the case in which both the l and the n are blocked: _u_ch. In terms of conditional probabilities relevant to perception, what is the effect of blocking out the l, n, and both?\n",
    "\n",
    "**Bunch  \n",
    "Munch  \n",
    "Hunch (Vorahnung)  \n",
    "Punch  \n",
    "Runch (Ackerrettich)**\n",
    "\n",
    "**Conditional probabilities form transitions from sensation to perception. Blocking out certain letters transforms the sensation which can alter the perception. In detail, different world states are compared, which are subjects in likelihood functions helping to perceive the letters by interpreting their potential meaning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96270dab-60b5-4197-8157-529ef633feb7",
   "metadata": {},
   "source": [
    "### Problem 1.9 The Easter bunny in October\n",
    "\n",
    "“Whereas on Easter the drawing was significantly more often recognized as a bunny, in October it was considered a bird by most subjects.”\n",
    "To solve this mystery applying Bayes’ rule, the prior mindset  combined with  estimated likelihood of observing a certain image  influence what is perceived. \n",
    "\n",
    "**In October, events, objects and animals (like ducks and duck roasts) related to autumn are easily accessible. This portrays a different prior and according likelihood than in April, when Easter is usually  happening making Easter bunnies more prominent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260fcab-75f4-40af-967c-7057ba8d5f4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem 1.11 Give three daily-life examples ...\n",
    "... (perceptual or cognitive) in which you tried to infer a world state from incomplete or imperfect information. For each example, specify the observation(s), the world state of interest, and the source(s) of uncertainty\n",
    "\n",
    "**I have two flatmates, A. and M. One morning I woke up noticing one of them lying next to me (observation). I thought oh yeah that must be M., because she likes to sleep next to me (inferred world state) without investigating any further(source of uncertainty). I went on with  my morning routine only to be surprised that A. and not M. came out of my room. \n",
    "I dog-sitted the other day and every time there was a noise (observation) on the other side of the closed apartment door (source of uncertainty) the dog started barking because it expected danger or threat (inferred world state).\n",
    "When I come home, I can see the bathroom window from outside the apartment. If there is light (observation) I assume that my flatmate is home (inferred world state) without me having talked to or seen her (source of uncertainty).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4a7ad-a7eb-4ed4-a933-643853ebb52f",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "\n",
    "### Summary\n",
    "In this chapter, we have introduced the precise formulation of Bayes’ rule and applied it to a\n",
    "range of discrete estimation problems. We have seen how Bayes’ rule makes concrete, meaningful\n",
    "statements about probabilities possible. Regarding the use of Bayes’ rule, we have learned the\n",
    "following:\n",
    "* Bayesian modeling starts with a model of the statistical structure of the world and the\n",
    "observations: the <mark>generative model</mark>.\n",
    "* Conditional probabilities are not symmetrical. In general, $p(A|B) \\ne p(B|A)$.\n",
    "* Bayes’ rule calculates the probabilities of hypotheses conditioned on the observation – the\n",
    "<mark>posterior probabilities, $p(H|obs)$</mark> – from the probabilities of the observation conditioned on\n",
    "the hypotheses – the likelihoods, $p(obs|H)$ – and the prior probabilities, $p(H)$.\n",
    "* $p(obs)$ in Bayes’ rule normalizes the probability and can be rewritten as <mark>$p(obs) = ∑H p(obs|H)p(H)$</mark>.\n",
    "* <mark>Priors do not always play the main role</mark> in Bayesian models. In some problems, such as the\n",
    "Gestalt example, the prior is relatively unimportant and the likelihood dominates.\n",
    "* In some scenarios, priors and/or likelihoods may change over time.\n",
    "* Bayesian inference is a component of deriving the optimal strategy in any inference task.\n",
    "However, not all Bayesian inference is optimal.\n",
    "* Finding evidence for (near-)optimal behavior does not necessarily mean that the components\n",
    "of the Bayesian model are internally represented in the brain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4268b-757f-46be-a2e2-c43ba15f3895",
   "metadata": {},
   "source": [
    "### Problem 2.1 Think of three daily-life examples ...\n",
    "... of random variables $A$ and $B$ for which intuitively, $p(A|B) \\ne p(B|A)$. In each case, state which probability is greater, and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81805a-5ca9-4850-9af4-4525405619fc",
   "metadata": {},
   "source": [
    "1) Pitbull-type dogs are not amongst the most popular dogs in Vienna, but there are certain districts in which chances are higher that a random dog that you see on the street is a Pitbull. Let's consider the 12th district, Meidling: Though it is one of the districts with the lowest dog population per resident ($p(Dog)$ is low), if you happen to see a dog in this district, the probability for \"Pitbull\" given the observation \"Dog\" ($p(Pitbull | Dog)$) is the highest among all districts in Vienna. What is the probability, that you see this random dog on the street given it is a pitbull type ($p(Dog | Pitbull)$)? It is low, even lower than $p(Dog)$ since Pitbulls are a subtype of Dog.\n",
    "\n",
    "2)  p(speech impediment | lesion in broca area) > p(lesion in broca area | speech impediment)\n",
    "Someone with lesion in brocas area is very likely ot have a speech impediment which doesn't mean that everyone with a speech impediment has a lesion. \n",
    "\n",
    "\n",
    "3)  p(loud bang | violent car crash) > p(violent car crash | loud bang)\n",
    "Every hard crash will create a lot of noise. Not every form of noise is a car crash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce957e-616c-4506-a3f6-5bf290400b03",
   "metadata": {},
   "source": [
    "### Problem 2.2 Imagine you have collected data ...\n",
    "... about reported sightings of the dodo throughout\n",
    "history. We will call these data S. Suppose you are interested in the time the dodo went extinct,\n",
    "denoted E. Then the likelihood function of interest to you is\n",
    "a) p(E|S) as a function of S\n",
    "b) p(E|S) as a function of E\n",
    "c) p(S|E) as a function of S\n",
    "d) p(S|E) as a function of E\n",
    "\n",
    "**c) p(S|E) as a function of S**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e98ce4-5c48-4744-94dd-6570fb496f14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem 2.3 In early July 2021, following large ...\n",
    "... gatherings associated with Independence Day festivities, hundreds of people in Provincetown, MA, USA, became infected with the virus that\n",
    "causes COVID-19. According to an article published in the Washington Post (C.Y. Johnson, July\n",
    "30, 2021), “A sobering scientific analysis...found that three-quarters of the people infected... were\n",
    "fully vaccinated.” The article does go on to emphasize that infected fully vaccinated individuals are\n",
    "very unlikely to suffer severe illness. Nevertheless, the quoted statement understandably alarmed\n",
    "many readers, as it suggested that the vaccines against COVID-19 were ineffective at preventing\n",
    "infection. A crucial piece of information, not provided in the article, was that a large majority of\n",
    "people in Provincetown were fully vaccinated against COVID-19. Interpret these data, and the\n",
    "alarm readers may have felt, in light of the prosecutor’s fallacy.\n",
    "\n",
    "**Since the majority is vaccinated the $p(vaccinated)$ is high, $p(vaccinated|infected)$ is lower, but still high - simpy because of the proportions in the population.**\n",
    "\n",
    "**The important piece is: $p(infected|vaccinated)$ < $p(infected|unvaccinated)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f6f7ac-9661-45ec-9259-56dc75a6574b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problem 2.4 At a particular university, ...\n",
    "... 15% of all students are in humanities, 55% of all students\n",
    "are undergrads, and 18% of undergrads are in humanities. What is the probability that a random\n",
    "humanities student is an undergrad?\n",
    "\n",
    "**H ... Humanities  \n",
    "U ... Undergrads**\n",
    "\n",
    "\n",
    "$p(H) = 0.15$   \n",
    "$p(U) = 0.55$   \n",
    "$p(H | U) = 0.18$\n",
    "\n",
    "**Bayes-Rule:**\n",
    "\n",
    "$p(U | H) = \\frac{p(H | U)}{p(H)} p(U) = \\frac{0.18}{0.15} 0.55 = 0.458333$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662e524a-2ddb-4ee7-9a67-513a4d6124ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45833333333333337"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".15 * .55 / .18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a2ff7-57ad-48c8-ba10-fa2b2dad119f",
   "metadata": {},
   "source": [
    "### 2.6 Intuitive explanation why likelihoods do not need to sum to 1\n",
    "\n",
    "There is a 100% chance for any possible world state to occur (some world state has to occur). Same goes for the posteriors. A likelihood on the other hand describes a conditional probability of a observation. It’s complement isn’t concerned with a matching world state but with a joint observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab4196-3946-47c2-8e0f-322a2c900155",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.7 Posterior is equal to prior, if the likelihood function is perfectly flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f7548-6131-43ea-a65f-197340f22286",
   "metadata": {},
   "source": [
    "The posterior of any given prior Pk is:\n",
    "\n",
    "$Posterior = \\frac{Likelihood(k)*Prior(k)}{(Likelihood(k)*Prior(k)+Likelihood(n)*Prior(n)} = \\frac{Pk*Lk}{Pk*Lk+Pn*Ln}$\n",
    "\n",
    "Since all Likelihoods are equal:\n",
    "\n",
    "$\\frac{Pk*L}{Pk*L+Pn*L}$\n",
    "\n",
    "Which can be written as:\n",
    "\n",
    "$\\frac{Pk*L}{L(Pk+Pn)}$\n",
    "\n",
    "Reducing the fraction gives:\n",
    "\n",
    "$\\frac{Pk}{Pk+Pn}$\n",
    "\n",
    "And since the sum of all priors = 1:\n",
    "\n",
    "$\\frac{Pk}{Pk+Pn} = \\frac{Pk}{1} = Pk$ \n",
    "\n",
    "Therefore prior = posterior for identical likelihoods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
