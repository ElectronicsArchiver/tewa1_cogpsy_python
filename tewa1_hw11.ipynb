{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7836b7e4-afb3-4af3-9c89-a09b94862926",
   "metadata": {
    "id": "aAmuxCstDaa3"
   },
   "source": [
    "homework 11\n",
    "===\n",
    "due: June 15 2022\n",
    "\n",
    "authors: pegler, prüwasser, scheftner\n",
    "\n",
    "---\n",
    "Read chapter 1-2 of Bayesian models of perception and action.\n",
    "\n",
    "https://www.cns.nyu.edu/malab/static/files/Bayesian_models_of_perception_and_action_v3.pdf\n",
    "\n",
    "Choose 6 problems from the end of the chapter, for both chapters, and submit the solution as text here.\n",
    "\n",
    "No need to write long explanations, try to be brief!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef562f40-8684-43f5-963b-cffc40e56d4e",
   "metadata": {},
   "source": [
    "## Chapter 1\n",
    "\n",
    "### Summary\n",
    "In this chapter, we have introduced the concept that perception is inherently probabilistic, and as\n",
    "such optimally characterized as a process of Bayesian inference. Regarding Bayesian inference, we\n",
    "have learned the following:\n",
    "\n",
    "* Conditional probabilities such as $p(A | B)$ represents the probability of $A$ given $B$. In Bayesian\n",
    "perceptual inference, $A$ and $B$ typically represent a world state and an observation.\n",
    "* The likelihood function, $p(observation | world state)$, captures the information content of the\n",
    "sensory observation, relevant to distinguishing one world state from another.\n",
    "* <mark>The flatter the likelihood function, the less we learn from our senses</mark>. If the likelihood\n",
    "function is perfectly flat, then the observer has learned nothing from the observation.\n",
    "* In some cases, such as the “Is that my friend?” example, the likelihood changes over time.\n",
    "* <mark>The prior distribution over world states, $p(world state)$, summarizes the information content\n",
    "of our past observations</mark>, the background knowledge we have about the world. Perception\n",
    "is not based entirely on sensory observation, but also on expectation grounded in previous\n",
    "experience.\n",
    "* <mark>Flatter prior distributions mean we know less</mark> about the potential states of the world.\n",
    "* Bayes’ rule calculates the posterior probability of each hypothesized world state, $p(world state | observation)$,\n",
    "from the likelihoods and prior probabilities of the world states.\n",
    "* The procedures of Bayesian inference apply equally to situations in which the hypothesized\n",
    "world states are <mark>discrete</mark> or in which they are <mark>continuous</mark>.\n",
    "* Perceptual situations, whether in vision, audition, or other senses, are subject to various\n",
    "levels of <mark>uncertainty</mark>.\n",
    "* Speech perception is fraught with phonetic and syntactic ambiguity, frequently giving rise to\n",
    "flat likelihood functions. The combination of priors and likelihoods can cause misinterpretations such as mondegreens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6f6c0-434d-44a0-8226-c0a585036f17",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "(1.2)\n",
    "\n",
    "Why is it that we identify ourselves at the very beginning of a phone conversation,\n",
    "even to people we already know, but we do not do this when we meet in person? Express your\n",
    "answer within the framework of Bayesian perceptual inference.\n",
    "\n",
    "**We know that in phone calls perception is limited to the auditory system and the quality of sensory data could be further limited by a bad phone connection. This potential noisiness we can counteract by telling the other person who we are to reduce uncertainty and widen the likelihood function. When we meet someone in person we have much more and less ambigious sensory data at hand. From afar it might be still ambigious, but once we meet and see the other person from 2 or 3 meter distance we can be very certain that the other person recognizes us.**\n",
    "\n",
    "&\n",
    "\n",
    "**Person can fungate as prior and therefor influence perceived likelihood. Identifying oneself sets ankers for selection of priors (contexts), which will even the way of understanding correctly what a person might have said (likelihood) and therefor perceiving correctly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31317b8b-d35c-4930-90a7-0bd4ba944695",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "(1.4)\n",
    "\n",
    "To explore how a noisy environment engenders uncertainty, consider the word “lunch.” Suppose that you see this word written (or hear it spoken), with the letter “l” blocked out (making it unknown): _unch (e.g., by ambient auditory noise). List all source words that are compatible with what you see. Now consider the case in which both the l and the n are blocked: _u_ch. In terms of conditional probabilities relevant to perception, what is the effect of blocking out the l, n, and both?\n",
    "\n",
    "**Bunch  \n",
    "Munch  \n",
    "Hunch (Vorahnung)  \n",
    "Punch  \n",
    "Runch (Ackerrettich)**\n",
    "\n",
    "**Conditional probabilities form transitions from sensation to perception. Blocking out certain letters transforms the sensation which can alter the perception. In detail, different world states are compared, which are subjects in likelihood functions helping to perceive the letters by interpreting their potential meaning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96270dab-60b5-4197-8157-529ef633feb7",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "(1.6)\n",
    "\n",
    "English speakers sometimes incorrectly perceive English words when they listen\n",
    "to songs sung in a foreign language with which they are unfamiliar, and listeners also mistakenly\n",
    "perceive words in music that is played backwards. Provide a Bayesian explanation for these\n",
    "phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260fcab-75f4-40af-967c-7057ba8d5f4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem 4\n",
    "\n",
    "(1.8)\n",
    "\n",
    "Suppose you see someone you do not know, getting only a brief look at them from\n",
    "a distance of about 10 meters. If you are interested in estimating this person’s age, how would\n",
    "you proceed? What factors, including and in addition to the person’s appearance, would affect\n",
    "your estimation? Provide a Bayesian description of your reasoning. As part of your answer, draw\n",
    "examples of your likelihood function, prior distribution, and resulting posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01bab2-8f28-4119-9984-92addbc0e7b7",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b9774-2d8e-4b42-8630-0f52f3054650",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4a7ad-a7eb-4ed4-a933-643853ebb52f",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "\n",
    "### Summary\n",
    "In this chapter, we have introduced the precise formulation of Bayes’ rule and applied it to a\n",
    "range of discrete estimation problems. We have seen how Bayes’ rule makes concrete, meaningful\n",
    "statements about probabilities possible. Regarding the use of Bayes’ rule, we have learned the\n",
    "following:\n",
    "* Bayesian modeling starts with a model of the statistical structure of the world and the\n",
    "observations: the <mark>generative model</mark>.\n",
    "* Conditional probabilities are not symmetrical. In general, $p(A|B) \\ne p(B|A)$.\n",
    "* Bayes’ rule calculates the probabilities of hypotheses conditioned on the observation – the\n",
    "<mark>posterior probabilities, $p(H|obs)$</mark> – from the probabilities of the observation conditioned on\n",
    "the hypotheses – the likelihoods, $p(obs|H)$ – and the prior probabilities, $p(H)$.\n",
    "* $p(obs)$ in Bayes’ rule normalizes the probability and can be rewritten as <mark>$p(obs) = ∑H p(obs|H)p(H)$</mark>.\n",
    "* <mark>Priors do not always play the main role</mark> in Bayesian models. In some problems, such as the\n",
    "Gestalt example, the prior is relatively unimportant and the likelihood dominates.\n",
    "* In some scenarios, priors and/or likelihoods may change over time.\n",
    "* Bayesian inference is a component of deriving the optimal strategy in any inference task.\n",
    "However, not all Bayesian inference is optimal.\n",
    "* Finding evidence for (near-)optimal behavior does not necessarily mean that the components\n",
    "of the Bayesian model are internally represented in the brain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4268b-757f-46be-a2e2-c43ba15f3895",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "(2.1)\n",
    "\n",
    "Think of three daily-life examples of random variables A and B for which intuitively,\n",
    "p(A|B) ̸ = p(B|A). In each case, state which probability is greater, and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce957e-616c-4506-a3f6-5bf290400b03",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "(2.4)\n",
    "\n",
    "Problem 2.4 At a particular university, 15% of all students are in humanities, 55% of all students\n",
    "are undergrads, and 18% of undergrads are in humanities. What is the probability that a random\n",
    "humanities student is an undergrad?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e98ce4-5c48-4744-94dd-6570fb496f14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem 3\n",
    "\n",
    "(2.3)\n",
    "\n",
    "In early July 2021, following large gatherings associated with Independence Day\n",
    "festivities, hundreds of people in Provincetown, MA, USA, became infected with the virus that\n",
    "causes COVID-19. According to an article published in the Washington Post (C.Y. Johnson, July\n",
    "30, 2021), “A sobering scientific analysis...found that three-quarters of the people infected... were\n",
    "fully vaccinated.” The article does go on to emphasize that infected fully vaccinated individuals are\n",
    "very unlikely to suffer severe illness. Nevertheless, the quoted statement understandably alarmed\n",
    "many readers, as it suggested that the vaccines against COVID-19 were ineffective at preventing\n",
    "infection. A crucial piece of information, not provided in the article, was that a large majority of\n",
    "people in Provincetown were fully vaccinated against COVID-19. Interpret these data, and the\n",
    "alarm readers may have felt, in light of the prosecutor’s fallacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f6f7ac-9661-45ec-9259-56dc75a6574b",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab4196-3946-47c2-8e0f-322a2c900155",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a2ff7-57ad-48c8-ba10-fa2b2dad119f",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b462f-d9d5-4335-b7d7-3f7593d63f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
